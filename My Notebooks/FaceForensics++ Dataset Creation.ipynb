{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================================================\n# STEP 0: INSTALL\n# =========================================================\n!pip install -q timm retina-face opencv-python albumentations tqdm scikit-learn\n\n# =========================================================\n# STEP 1: DOWNLOAD FACEFORENSICS++\n# =========================================================\n!wget -q https://kaldir.vc.in.tum.de/faceforensics_download_v4.py\n!sed -i \"s/_ = input('')/# _ = input('')/g\" faceforensics_download_v4.py\n\n# REAL\n!python faceforensics_download_v4.py /kaggle/working/ffpp \\\n -d original \\\n -c c23 \\\n -t videos \\\n -n 200 \\\n --server EU2\n\n# FAKE\n!python faceforensics_download_v4.py /kaggle/working/ffpp \\\n -d Deepfakes \\\n -c c23 \\\n -t videos \\\n -n 200 \\\n --server EU2\n\n# =========================================================\n# STEP 2: IMPORTS\n# =========================================================\nimport os, cv2, torch, timm\nimport numpy as np\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom retinaface import RetinaFace\nfrom torchvision import transforms\n\n# =========================================================\n# STEP 3: CONFIG\n# =========================================================\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMG_SIZE = 224\nBATCH_SIZE = 16\nEPOCHS = 3\n\nREAL_VIDEOS = \"/kaggle/working/ffpp/original_sequences/youtube/c23/videos\"\nFAKE_VIDEOS = \"/kaggle/working/ffpp/manipulated_sequences/Deepfakes/c23/videos\"\n\nFACE_DATA = \"/kaggle/working/faces\"\nos.makedirs(FACE_DATA, exist_ok=True)\n\n# =========================================================\n# STEP 4: FACE ALIGNMENT\n# =========================================================\ndef align_and_crop(img, facial_area, landmarks):\n    x1,y1,x2,y2 = facial_area\n    le = landmarks[\"left_eye\"]\n    re = landmarks[\"right_eye\"]\n\n    dx, dy = re[0]-le[0], re[1]-le[1]\n    angle = np.degrees(np.arctan2(dy, dx))\n    center = ((x1+x2)//2, (y1+y2)//2)\n\n    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n    aligned = cv2.warpAffine(img, M, img.shape[1::-1])\n    face = aligned[y1:y2, x1:x2]\n\n    return cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n\n# =========================================================\n# STEP 5: FACE EXTRACTION\n# =========================================================\ndef extract_faces(video_path, out_dir, every_n=10):\n    os.makedirs(out_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    idx = saved = 0\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if idx % every_n == 0:\n            faces = RetinaFace.detect_faces(frame)\n            if faces:\n                face = max(\n                    faces.values(),\n                    key=lambda f:(f[\"facial_area\"][2]-f[\"facial_area\"][0]) *\n                                 (f[\"facial_area\"][3]-f[\"facial_area\"][1])\n                )\n                crop = align_and_crop(frame, face[\"facial_area\"], face[\"landmarks\"])\n                cv2.imwrite(f\"{out_dir}/{saved:05d}.jpg\", crop)\n                saved += 1\n        idx += 1\n\n    cap.release()\n\n# =========================================================\n# STEP 6: PROCESS VIDEOS\n# =========================================================\nfor cls, src_dir in [(\"real\", REAL_VIDEOS), (\"fake\", FAKE_VIDEOS)]:\n    dst_root = f\"{FACE_DATA}/{cls}\"\n    os.makedirs(dst_root, exist_ok=True)\n\n    for vid in tqdm(os.listdir(src_dir), desc=f\"Extracting {cls} faces\"):\n        extract_faces(\n            f\"{src_dir}/{vid}\",\n            f\"{dst_root}/{vid.replace('.mp4','')}\"\n        )\n\n# =========================================================\n# STEP 7: DATASET\n# =========================================================\nclass FaceDataset(Dataset):\n    def __init__(self, root):\n        self.data = []\n        for label, cls in enumerate([\"real\",\"fake\"]):\n            for vid in os.listdir(f\"{root}/{cls}\"):\n                for img in os.listdir(f\"{root}/{cls}/{vid}\"):\n                    self.data.append((f\"{root}/{cls}/{vid}/{img}\", label))\n\n        self.tf = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.5]*3, [0.5]*3)\n        ])\n\n    def __len__(self): return len(self.data)\n\n    def __getitem__(self, i):\n        path,label = self.data[i]\n        return self.tf(Image.open(path).convert(\"RGB\")), label\n\n# =========================================================\n# STEP 8: SWIN MODEL\n# =========================================================\nclass SwinDF(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n            \"swin_base_patch4_window7_224\",\n            pretrained=True,\n            num_classes=0\n        )\n        self.fc = nn.Linear(self.backbone.num_features, 2)\n\n    def forward(self, x):\n        return self.fc(self.backbone(x))\n\n# =========================================================\n# STEP 9: TRAIN\n# =========================================================\ndataset = FaceDataset(FACE_DATA)\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nmodel = SwinDF().to(DEVICE)\nopt = torch.optim.AdamW(model.parameters(), lr=2e-4)\nloss_fn = nn.CrossEntropyLoss()\n\nfor ep in range(EPOCHS):\n    correct = total = 0\n    model.train()\n\n    for x,y in tqdm(loader, desc=f\"Epoch {ep+1}/{EPOCHS}\"):\n        x,y = x.to(DEVICE), y.to(DEVICE)\n        opt.zero_grad()\n        out = model(x)\n        loss = loss_fn(out,y)\n        loss.backward()\n        opt.step()\n\n        correct += (out.argmax(1)==y).sum().item()\n        total += y.size(0)\n\n    print(f\"Epoch {ep+1} Accuracy: {correct/total:.4f}\")\n\nprint(\"âœ… DONE â€” Face-based Swin deepfake detector trained\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# STEP 0: INSTALL DEPENDENCIES\n# =========================================================\n!pip install -q timm retina-face opencv-python albumentations tqdm scikit-learn\n\n# =========================================================\n# STEP 1: DOWNLOAD FACEFORENSICS++\n# =========================================================\n!wget -q https://kaldir.vc.in.tum.de/faceforensics_download_v4.py\n!sed -i \"s/_ = input('')/# _ = input('')/g\" faceforensics_download_v4.py\n\n# REAL videos\n\n\n# =========================================================\n# STEP 2: IMPORTS\n# =========================================================\nimport os, cv2, torch, timm\nimport numpy as np\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom retinaface import RetinaFace\nfrom torchvision import transforms\n\n# =========================================================\n# STEP 3: CONFIG\n# =========================================================\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMG_SIZE = 224\nBATCH_SIZE = 16\nEPOCHS = 3   # increase later for better accuracy\n\nRAW_DATA = \"/kaggle/working/ffpp\"\nFACE_DATA = \"/kaggle/working/faces\"\nos.makedirs(FACE_DATA, exist_ok=True)\n\nREAL_DIR = f\"{RAW_DATA}/original_sequences/youtube/c23/videos\"\nFAKE_DIR = f\"{RAW_DATA}/manipulated_sequences/Deepfakes/c23/videos\"\n\n# =========================================================\n# STEP 4: FACE ALIGNMENT (FIXED)\n# =========================================================\ndef align_and_crop(img, facial_area, landmarks):\n    x1, y1, x2, y2 = map(int, facial_area)\n\n    left_eye  = landmarks[\"left_eye\"]\n    right_eye = landmarks[\"right_eye\"]\n\n    dx = right_eye[0] - left_eye[0]\n    dy = right_eye[1] - left_eye[1]\n    angle = np.degrees(np.arctan2(dy, dx))\n\n    cx = int((x1 + x2) / 2)\n    cy = int((y1 + y2) / 2)\n    center = (cx, cy)\n\n    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n    aligned = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n\n    face = aligned[y1:y2, x1:x2]\n    if face.size == 0:\n        return None\n\n    return cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n\n# =========================================================\n# STEP 5: FACE EXTRACTION\n# =========================================================\ndef extract_faces(video_path, out_dir, every_n=10):\n    os.makedirs(out_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    frame_idx = saved = 0\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_idx % every_n == 0:\n            faces = RetinaFace.detect_faces(frame)\n            if faces:\n                face = max(\n                    faces.values(),\n                    key=lambda f: (f[\"facial_area\"][2]-f[\"facial_area\"][0]) *\n                                  (f[\"facial_area\"][3]-f[\"facial_area\"][1])\n                )\n                crop = align_and_crop(frame, face[\"facial_area\"], face[\"landmarks\"])\n                if crop is not None:\n                    cv2.imwrite(f\"{out_dir}/{saved:05d}.jpg\", crop)\n                    saved += 1\n\n        frame_idx += 1\n\n    cap.release()\n\n# =========================================================\n# STEP 6: PROCESS VIDEOS â†’ FACE DATASET\n# =========================================================\nfor cls, src_dir in [(\"real\", REAL_DIR), (\"fake\", FAKE_DIR)]:\n    dst_root = f\"{FACE_DATA}/{cls}\"\n    os.makedirs(dst_root, exist_ok=True)\n\n    for vid in tqdm(os.listdir(src_dir), desc=f\"Extracting {cls} faces\"):\n        extract_faces(\n            f\"{src_dir}/{vid}\",\n            f\"{dst_root}/{vid.replace('.mp4','')}\"\n        )\n\n# =========================================================\n# STEP 7: DATASET\n# =========================================================\nclass FaceDataset(Dataset):\n    def __init__(self, root):\n        self.samples = []\n        for label, cls in enumerate([\"real\", \"fake\"]):\n            for vid in os.listdir(f\"{root}/{cls}\"):\n                for img in os.listdir(f\"{root}/{cls}/{vid}\"):\n                    self.samples.append((f\"{root}/{cls}/{vid}/{img}\", label))\n\n        self.tf = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.5]*3, [0.5]*3)\n        ])\n\n    def __len__(self): return len(self.samples)\n\n    def __getitem__(self, i):\n        path, label = self.samples[i]\n        img = Image.open(path).convert(\"RGB\")\n        return self.tf(img), label\n\n# =========================================================\n# STEP 8: SWIN TRANSFORMER\n# =========================================================\nclass SwinDeepfake(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n            \"swin_base_patch4_window7_224\",\n            pretrained=True,\n            num_classes=0\n        )\n        self.head = nn.Linear(self.backbone.num_features, 2)\n\n    def forward(self, x):\n        return self.head(self.backbone(x))\n\n# =========================================================\n# STEP 9: TRAIN\n# =========================================================\ndataset = FaceDataset(FACE_DATA)\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nmodel = SwinDeepfake().to(DEVICE)\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total, correct = 0, 0\n\n    for x, y in tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n\n        correct += (out.argmax(1) == y).sum().item()\n        total += y.size(0)\n \n    print(f\"Epoch {epoch+1} Accuracy: {correct/total:.4f}\")\n\nprint(\"âœ… TRAINING COMPLETE â€” SWIN TRANSFORMER DEEPFAKE MODEL READY\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FaceDataset(Dataset):\n    def __init__(self, root):\n        self.samples = []\n        for label, cls in enumerate([\"real\", \"fake\"]):\n            for vid in os.listdir(f\"{root}/{cls}\"):\n                for img in os.listdir(f\"{root}/{cls}/{vid}\"):\n                    self.samples.append((f\"{root}/{cls}/{vid}/{img}\", label))\n\n        self.tf = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.5]*3, [0.5]*3)\n        ])\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, label = self.samples[idx]\n        img = Image.open(path).convert(\"RGB\")\n        return self.tf(img), label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SwinDF(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n            \"swin_tiny_patch4_window7_224\",\n            pretrained=True,\n            num_classes=0\n        )\n        self.head = nn.Linear(self.backbone.num_features, 2)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        return self.head(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SwinDF().to(DEVICE)\n\nfor p in model.backbone.parameters():\n    p.requires_grad = False\n\nfor p in model.head.parameters():\n    p.requires_grad = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r faces_backup.zip /kaggle/working/faces","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# STEP 0: INSTALL\n# =========================================================\n!pip install -q timm retina-face opencv-python tqdm albumentations\n\n# =========================================================\n# STEP 1: IMPORTS\n# =========================================================\nimport os, cv2, torch, timm\nimport numpy as np\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom retinaface import RetinaFace\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# =========================================================\n# STEP 2: CONFIG\n# =========================================================\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMG_SIZE = 224\nBATCH_SIZE = 8          # SAFE for Kaggle GPU\nEPOCHS = 3\nFRAMES_PER_VIDEO = 10\n\nRAW_ROOT = \"/kaggle/working/ffpp\"\nFACE_ROOT = \"/kaggle/working/faces\"\nos.makedirs(FACE_ROOT, exist_ok=True)\n\n# =========================================================\n# STEP 3: SAFE FACE CROP (NO ROTATION)\n# =========================================================\ndef crop_face(frame, facial_area):\n    h, w, _ = frame.shape\n    x1, y1, x2, y2 = facial_area\n\n    x1 = max(0, x1)\n    y1 = max(0, y1)\n    x2 = min(w, x2)\n    y2 = min(h, y2)\n\n    face = frame[y1:y2, x1:x2]  # âœ… CORRECT ORDER\n    if face.size == 0:\n        return None\n\n    face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n    return face\n\n# =========================================================\n# STEP 4: EXTRACT FACES FROM VIDEOS\n# =========================================================\ndef extract_faces(video_path, out_dir):\n    os.makedirs(out_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    step = max(total // FRAMES_PER_VIDEO, 1)\n\n    idx = saved = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if idx % step == 0:\n            faces = RetinaFace.detect_faces(frame)\n            if faces:\n                face = max(\n                    faces.values(),\n                    key=lambda f: (f[\"facial_area\"][2]-f[\"facial_area\"][0]) *\n                                  (f[\"facial_area\"][3]-f[\"facial_area\"][1])\n                )\n                crop = crop_face(frame, face[\"facial_area\"])\n                if crop is not None:\n                    cv2.imwrite(f\"{out_dir}/{saved}.jpg\", crop)\n                    saved += 1\n        idx += 1\n    cap.release()\n\n# =========================================================\n# STEP 5: PROCESS FF++\n# =========================================================\npaths = {\n    \"real\": f\"{RAW_ROOT}/original_sequences/youtube/c23/videos\",\n    \"fake\": f\"{RAW_ROOT}/manipulated_sequences/Deepfakes/c23/videos\"\n}\n\nfor cls in [\"real\", \"fake\"]:\n    dst = f\"{FACE_ROOT}/{cls}\"\n    os.makedirs(dst, exist_ok=True)\n\n    for vid in tqdm(os.listdir(paths[cls]), desc=f\"Extracting {cls}\"):\n        extract_faces(\n            f\"{paths[cls]}/{vid}\",\n            f\"{dst}/{vid.replace('.mp4','')}\"\n        )\n\nprint(\"âœ… FACE DATASET READY\")\n\n# =========================================================\n# STEP 6: DATASET\n# =========================================================\nclass FaceDataset(Dataset):\n    def __init__(self, root):\n        self.samples = []\n        for label, cls in enumerate([\"real\", \"fake\"]):\n            for vid in os.listdir(f\"{root}/{cls}\"):\n                for img in os.listdir(f\"{root}/{cls}/{vid}\"):\n                    self.samples.append((f\"{root}/{cls}/{vid}/{img}\", label))\n\n        self.tf = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.5]*3, [0.5]*3)\n        ])\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, i):\n        path, label = self.samples[i]\n        img = Image.open(path).convert(\"RGB\")\n        return self.tf(img), label\n\ndataset = FaceDataset(FACE_ROOT)\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# =========================================================\n# STEP 7: SWIN-TINY (NO OOM)\n# =========================================================\nclass SwinDF(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n            \"swin_tiny_patch4_window7_224\",\n            pretrained=True,\n            num_classes=0\n        )\n        self.head = nn.Linear(self.backbone.num_features, 2)\n\n    def forward(self, x):\n        return self.head(self.backbone(x))\n\nmodel = SwinDF().to(DEVICE)\n\n# Freeze backbone for stability + memory\nfor p in model.backbone.parameters():\n    p.requires_grad = False\n\noptimizer = torch.optim.AdamW(model.head.parameters(), lr=2e-4)\ncriterion = nn.CrossEntropyLoss()\n\n# =========================================================\n# STEP 8: TRAIN\n# =========================================================\nfor epoch in range(EPOCHS):\n    model.train()\n    correct = total = 0\n\n    for x, y in tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n\n        correct += (out.argmax(1) == y).sum().item()\n        total += y.size(0)\n\n    print(f\"Epoch {epoch+1} Accuracy: {correct/total:.4f}\")\n\nprint(\"ðŸŽ¯ TRAINING COMPLETE\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}